{
  "ideas": [
    {
      "id": "idea_001",
      "title": "Add Batch Normalization after each Conv layer",
      "hypothesis": "The current SmallCNN has no normalization, causing training instability and slow convergence. Adding BatchNorm after each Conv2d (before ReLU) should reduce internal covariate shift, allow higher learning rates, and improve final accuracy.",
      "implementation_notes": "In example-research/model.py, add nn.BatchNorm2d(channels) after each nn.Conv2d and before each nn.ReLU. There are 5 Conv2d layers total: channels are 32, 32, 64, 64, 128. Remove the Dropout2d layers in features since BatchNorm provides its own regularization.",
      "files_to_modify": ["example-research/model.py"],
      "expected_impact": "high",
      "source": "arxiv:1502.03167",
      "status": "pending",
      "created_at": "2026-02-18T12:29:00Z"
    },
    {
      "id": "idea_002",
      "title": "Switch to Cosine Annealing learning rate schedule",
      "hypothesis": "The current training uses a fixed Adam LR of 0.001 throughout all 500 epochs. A cosine annealing schedule smoothly reduces the LR from 0.001 to near 0, which typically finds better optima by allowing large steps early and fine-grained steps late.",
      "implementation_notes": "In example-research/config.py, change LR_SCHEDULE from 'none' to 'cosine'. The train.py already has CosineAnnealingLR logic wired up in get_scheduler() â€” it just needs the config value changed. No model changes needed.",
      "files_to_modify": ["example-research/config.py"],
      "expected_impact": "medium",
      "source": "arxiv:1608.03983",
      "status": "pending",
      "created_at": "2026-02-18T12:29:00Z"
    }
  ]
}
