{
  "idea_id": "J5",
  "idea_title": "Warmup LR Schedule (Linear Warmup + ReduceLROnPlateau)",
  "git_branch": "experiment/J5-20260219_235220",
  "outcome": "regression",
  "smoke_test": {
    "status": "ok",
    "avg_seconds_per_epoch": 6.71,
    "threshold": 10
  },
  "metrics": {
    "primary": {
      "name": "accuracy",
      "value": 0.843
    },
    "supporting": {}
  },
  "vs_original_baseline": {
    "baseline_value": 0.84,
    "new_value": 0.843,
    "delta": 0.003,
    "delta_pct": 0.357,
    "improved": true
  },
  "vs_rolling_best": {
    "baseline_value": 0.851,
    "new_value": 0.843,
    "delta": -0.008,
    "delta_pct": -0.94,
    "improved": false
  },
  "branch_kept": false,
  "conclusion": "LR warmup (lr/10 to lr over 20 epochs) caused regression of -0.8pp vs rolling best (84.3% vs 85.1%). The warmup delays the optimizer reaching its target lr, slowing early-stage gradient learning for hard device pairs with focal loss. ReduceLROnPlateau already provides adaptive LR reduction \u2014 adding a warmup phase is redundant and counterproductive. lr=3e-4 from epoch 0 is confirmed optimal.",
  "id": "exp_049"
}